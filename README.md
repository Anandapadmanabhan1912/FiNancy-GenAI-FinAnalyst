# FiNancy: GenAI based Financial Reports and Document Analyst

An **Automated Research Agent** built using **Retrieval-Augmented Generation (RAG)** that connects **live web data** with **Large Language Models** to answer questions using **news published minutes ago** â€” not stale training data.

The application is built as a containerized, microservices-based system using FastAPI, Docker, and Docker Compose, enabling easy local development, scalability, and production-grade deployment.
This system acts as a **bridge between real-time information and generative AI**, ensuring answers are **grounded, factual, and source-aware**.

---

## ğŸš€ Key Features

- ğŸ” Live Web Research using DuckDuckGo
- ğŸ§  RAG Architecture to prevent hallucinations
- âš¡ Semantic Search with FAISS
- ğŸ’¾ Persistent Vector Memory
- ğŸŒ Streamlit Frontend
- ğŸ”Œ FastAPI Microservices
- ğŸ³ Dockerized Services
- ğŸ§© Docker Compose Orchestration
- ğŸ¤– Powered by Google Gemini 2.5 Flash

---
```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Streamlit UI        â”‚
â”‚          (Frontend)          â”‚
â”‚        Port: 8501            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ HTTP / REST
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     FastAPI Orchestrator     â”‚
â”‚      (API Gateway Layer)    â”‚
â”‚        Port: 8000            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚        â”‚        â”‚
        â”‚        â”‚        â”‚
        â–¼        â–¼        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Research â”‚ â”‚ Ingestionâ”‚ â”‚ Vector Store â”‚
â”‚ Service  â”‚ â”‚ Service  â”‚ â”‚  (FAISS)     â”‚
â”‚ (Search) â”‚ â”‚ (Scrape) â”‚ â”‚ Persistent   â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚            â”‚              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚     LLM Service      â”‚
          â”‚  Gemini 2.5 Flash    â”‚
          â”‚  (Answer Generator)  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```
---
## ğŸ—ï¸ System Architecture

The project is built on **four core pillars**:

| Component | Technology | Role |
|---------|-----------|------|
| **Interface** | Streamlit | Web dashboard, topic input, debug logs |
| **Orchestration** | LangChain | Connects tools, loaders, embeddings, and LLM |
| **Memory / Storage** | FAISS + Pickle | Vector similarity search & persistent storage |
| **LLM (Brain)** | Google Gemini 2.5 Flash | Reads retrieved data & generates answers |

---

##  How It Works (RAG Pipeline)

The agent follows a **4-stage pipeline** that transforms a user query into a **grounded conversational response**.

---

###  Automated Research (Discovery)

- Uses **DuckDuckGo Search Tool**
- Fetches top web snippets for the given topic
- Applies **custom Regex filtering** to extract the **top 2 clean URLs**
- Ensures only **high-relevance primary news sources** are used

ğŸ“Œ *Why?*  
Limits noise and improves factual precision.

---

### Document Ingestion (Scraping & Processing)

- Uses `UnstructuredURLLoader` to scrape article text
- Applies **RecursiveCharacterTextSplitter**

**Chunking Strategy:**
- Chunk size: **1000 characters**
- Overlap: **200 characters**


Preserves sentence continuity across chunks and prevents context loss.

---

###  Vectorization (Semantic Indexing)


- **Embedding Model:** `all-MiniLM-L6-v2` (HuggingFace)
- Each text chunk â†’ **numerical vector**
- Stored in **FAISS** for ultra-fast similarity search


Semantic understanding instead of keyword matching.

---

###  Retrieval & Answer Generation

When the user asks a question:

1. Question â†’ converted into a vector
2. FAISS retrieves the **most relevant chunks**
3. Retrieved chunks + question â†’ sent to **Gemini**
4. Gemini generates an answer **only using retrieved sources**

---

## ğŸ’¾ Persistence & Memory

- FAISS index is saved locally using `pickle.dump()`
- Stored as a `.pkl` file on disk
- On app restart:
  - Index is **reloaded**
  - Embedding function is **re-bound manually**

Solves a common FAISS bug where loaded indexes lose search capability.

---

## ğŸ” Authentication Strategy

- Bypasses **Google Application Default Credentials (ADC)**
- Explicitly passes `google_api_key` to the Gemini constructor

Works on any local machine  
---

## ğŸ› ï¸ Tech Stack

- **Python**
- **Streamlit**
- **LangChain**
- **FAISS**
- **HuggingFace Embeddings**
- **Google Gemini 2.5 Flash**
- **DuckDuckGo Search**
- **Pickle**

---

##  Use Cases

- Real-time news research
- Market & finance analysis
- Academic literature scanning
- Fact-checked GenAI chatbots
- Enterprise RAG systems
---

<img width="959" height="472" alt="image" src="https://github.com/user-attachments/assets/703c63ae-f8b4-4e31-8de1-091a016c1a4d" />
<img width="943" height="352" alt="image" src="https://github.com/user-attachments/assets/e6d1f405-3ddb-4b3a-8766-91b4522a01c6" />
<img width="870" height="389" alt="image" src="https://github.com/user-attachments/assets/4ff043e7-188c-4989-9151-0ee30b719b90" />
<img width="842" height="359" alt="image" src="https://github.com/user-attachments/assets/580baec5-736d-42e4-a4d0-3154fc9e3e73" />




## ğŸ“œ License

MIT License

---

## ğŸ‘¤ Author

**Anandapadmanabhan B**  
*GenAI | Full-Stack Web | React Native | Python Fullstack AI/ML*

---

â­ Please star !
